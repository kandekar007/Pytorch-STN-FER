{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install scikit-image -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Loading FER data ###\n",
    "device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n",
    "# if torch.cuda.is_available:\n",
    "#     print(\"CUDA is available.... 1 2 3...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./FER-2013/train.csv\")\n",
    "df = pd.read_csv(\"./FER-2013/fer2013/fer2013.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['pixelss']=[[int(y) for y in x.split()] for x in df['pixels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "      <th>pixelss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[70, 80, 82, 72, 58, 58, 60, 63, 54, 58, 60, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[151, 150, 147, 155, 148, 133, 111, 140, 170, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[231, 212, 156, 164, 174, 138, 161, 173, 182, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[24, 32, 36, 30, 32, 23, 19, 20, 30, 41, 21, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 15, 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage  \\\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training   \n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training   \n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training   \n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training   \n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training   \n",
       "\n",
       "                                             pixelss  \n",
       "0  [70, 80, 82, 72, 58, 58, 60, 63, 54, 58, 60, 4...  \n",
       "1  [151, 150, 147, 155, 148, 133, 111, 140, 170, ...  \n",
       "2  [231, 212, 156, 164, 174, 138, 161, 173, 182, ...  \n",
       "3  [24, 32, 36, 30, 32, 23, 19, 20, 30, 41, 21, 2...  \n",
       "4  [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 15, 23...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train=df[df['Usage']=='Training']\n",
    "df_valid=df[df['Usage']=='PrivateTest']\n",
    "df_test=df[df['Usage']=='PublicTest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im=np.array(df['pixelss'][6969])\n",
    "img=im.reshape(48,48)\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "part={}\n",
    "part['train']= list(range(0,len(df_train)))\n",
    "part['valid']= list(range(0,len(df_valid)))\n",
    "part['test']= list(range(0,len(df_test)))\n",
    "train_labels=df_train['emotion'].tolist()\n",
    "valid_labels=df_valid['emotion'].tolist()\n",
    "test_labels=df_test['emotion'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dff, transforms):\n",
    "        'Initialization'\n",
    "        self.transforms = transforms\n",
    "        self.dff=dff\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dff)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        #ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.dff.iloc[index]['pixelss']\n",
    "        X = np.array(X).reshape(48,48,1)\n",
    "        y = self.dff.iloc[index]['emotion']\n",
    "\n",
    "        if self.transforms:\n",
    "          X = self.transforms(X)\n",
    "        \n",
    "        X = torch.cat((X,X,X),0)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,'shuffle': True,'num_workers': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "from albumentations import pytorch as AT\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "#from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "\n",
    "class AlbumentationWrapper(object):\n",
    "    def __init__(self,split):\n",
    "        self.split=split\n",
    "        self.aug=albumentations.Compose([                                         \n",
    "    albumentations.Normalize((0.5), (0.5)),\n",
    "    AT.ToTensor()\n",
    "    ])\n",
    "\t\n",
    "        if self.split=='train':\n",
    "            self.aug=albumentations.Compose([\n",
    "                                             \n",
    "            #albumentations.Resize(48,48),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.Cutout(2,2,2,0.5),\n",
    "    albumentations.GaussNoise(),\n",
    "    #albumentations.ElasticTransform(),    \n",
    "    albumentations.Normalize((0.5), (0.5)),\n",
    "    AT.ToTensor()    \n",
    "    ])\n",
    "            \n",
    "    def __call__(self,img):\n",
    "        #img = np.array(img)\n",
    "        img = self.aug(image=img)['image']\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms , validation_transforms=AlbumentationWrapper('train'), AlbumentationWrapper('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(df_train, train_transforms)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(df_valid, validation_transforms)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "test_set = Dataset(df_test, validation_transforms)\n",
    "test_generator = data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_losses,train_acc,test_losses,test_acc, label):\n",
    "  fig, axs = plt.subplots(1,2,figsize=(20,8))\n",
    "  axs[0].plot(test_losses, label=label)\n",
    "  axs[0].set_title(\"Test Loss\")\n",
    "  axs[1].plot(test_acc, label=label)\n",
    "  axs[1].set_title(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer,scheduler):\n",
    "  model.train()\n",
    "  pbar = tqdm(train_loader)\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  processed = 0\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(pbar):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(data)\n",
    "    loss = criterion(y_pred, target)\n",
    "    running_loss += loss.item()\n",
    "    train_loss.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    processed += len(data)\n",
    "\n",
    "    #pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f} running_loss={running_loss} threshold={best_loss*(0.996)}')\n",
    "    train_acc.append(100*correct/processed)\n",
    "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} le={get_lr(optimizer)} Accuracy={100*correct/processed:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            new_target=target.view_as(pred)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    valid_loss.append(test_loss)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    valid_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Net(nn.Module):\n",
    "        \n",
    "    def __init__(self, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        dropout_value = dropout\n",
    "        # Input Block\n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.Dropout(dropout_value)\n",
    "        ) \n",
    "\n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.Dropout(dropout_value)            \n",
    "        ) \n",
    "\n",
    "        # TRANSITION BLOCK 1\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 24 RF=7\n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.Dropout(dropout_value)            \n",
    "        ) \n",
    "\n",
    "        self.convblock4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "        ) \n",
    "\n",
    "        self.convblock5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(1, 1), padding=1 , bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            # nn.Dropout(dropout_value)            \n",
    "        ) \n",
    "\n",
    "        # TRANSITION BLOCK 2\n",
    "        self.pool2 = nn.MaxPool2d(2, 2) # output_size = 12 RF=20\n",
    "\n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convblock6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            # nn.Dropout(dropout_value)            \n",
    "        ) \n",
    "\n",
    "        self.convblock7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            # nn.Dropout(dropout_value)            \n",
    "        )\n",
    "\n",
    "        # TRANSITION BLOCK 3\n",
    "        self.pool3 = nn.MaxPool2d(2, 2) # output_size =6 RF=32\n",
    "\n",
    "        self.convblock8 = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), padding=1, bias=False),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(512),\n",
    "             # nn.Dropout(dropout_value)            \n",
    "         ) \n",
    "\n",
    "        self.convblock9 = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(3, 3), padding=0, bias=False),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(256),\n",
    "             # nn.Dropout(dropout_value)            \n",
    "         )\n",
    "        # self.pool2 = nn.MaxPool2d(2, 2) # output_size = 2\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=4)\n",
    "        ) \n",
    "        self.convblock10 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=7, kernel_size=(1, 1), padding=0, bias=False)\n",
    "        ) \n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(8 * 8 * 32, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "        \n",
    "        #####################################################\n",
    "\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 8 * 8 * 32)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)            # transform the input\n",
    "        x = self.convblock1(x)     # Perform the usual forward pass\n",
    "        x = self.convblock2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblock3(x)        \n",
    "        x = self.convblock4(x)\n",
    "        x = self.convblock5(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.convblock6(x)\n",
    "        x = self.convblock7(x)\n",
    "        x = self.pool3(x)   \n",
    "        x = self.convblock8(x) \n",
    "        x = self.convblock9(x)    \n",
    "        x = self.gap(x)\n",
    "        x = self.convblock10(x)\n",
    "        x = x.view(-1, 7)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LR: 0.0019999999999999983 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=1.9494938850402832 Batch_id=1 le=0.0020000215288962613 Accuracy=10.16:   0%|          | 2/449 [01:08<4:01:44, 32.45s/it]"
     ]
    }
   ],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "model=Net(1)\n",
    "#model.to(device)\n",
    "epochs=32\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=9e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.02, steps_per_epoch=len(training_generator), pct_start=0.2, div_factor=10, cycle_momentum=False, epochs=epochs)\n",
    "\n",
    "input_size=(3,48,48)\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(\"EPOCH: %s LR: %s \" % (epoch, get_lr(optimizer)))\n",
    "    train(model, training_generator, optimizer,scheduler)\n",
    "    test(model, validation_generator)\n",
    "    #scheduler.step()\n",
    "plot(train_loss,train_acc, valid_loss, valid_acc, 'Loss & Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FerDataset(Dataset):\n",
    "    \"\"\"FER dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.frame.iloc[idx, 0].astype('str'))\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'emotion': emotion, 'pixels': pixels}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./FER-2013/train.csv\")\n",
    "df.head(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/home/naruto/Desktop/AI/STN/0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4da2547b1373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     print(i, sample['emotion'].shape, sample['pixels'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-acb15710a95b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                (plugin, kind))\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/home/naruto/Desktop/AI/STN/0'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "face_dataset = FerDataset(csv_file='./FER-2013/train.csv',\n",
    "                                    root_dir='.')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(face_dataset)):\n",
    "    sample = face_dataset[i]\n",
    "\n",
    "#     print(i, sample['emotion'].shape, sample['pixels'].shape)\n",
    "\n",
    "#     ax = plt.subplot(1, 4, i + 1)\n",
    "#     plt.tight_layout()\n",
    "#     ax.set_title('Sample #{}'.format(i))\n",
    "#     ax.axis('off')\n",
    "#     show_landmarks(**sample)\n",
    "\n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859b42573cde48cabfb16cd1c3916e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e21e1191d2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    transform=transforms.Compose([\n\u001b[1;32m      7\u001b[0m                        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1307\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.3081\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                    ])), batch_size=64, shuffle=True, num_workers=4)\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     69\u001b[0m             urllib.request.urlretrieve(\n\u001b[1;32m     70\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=64, shuffle=True, num_workers=4)\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])), batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depicting spatial transformer networks\n",
    "--------------------------------------\n",
    "\n",
    "Spatial transformer networks boils down to three main components :\n",
    "\n",
    "-  The localization network is a regular CNN which regresses the\n",
    "   transformation parameters. The transformation is never learned\n",
    "   explicitly from this dataset, instead the network learns automatically\n",
    "   the spatial transformations that enhances the global accuracy.\n",
    "-  The grid generator generates a grid of coordinates in the input\n",
    "   image corresponding to each pixel from the output image.\n",
    "-  The sampler uses the parameters of the transformation and applies\n",
    "   it to the input image.\n",
    "\n",
    ".. figure:: /_static/img/stn/stn-arch.png\n",
    "\n",
    ".. Note::\n",
    "   We need the latest version of PyTorch that contains\n",
    "   affine_grid and grid_sample modules.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        #self.fc2 = nn.\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 3 * 3, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "------------------\n",
    "\n",
    "Now, let's use the SGD algorithm to train the model. The network is\n",
    "learning the classification task in a supervised way. In the same time\n",
    "the model is learning STN automatically in an end-to-end fashion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(training_generator):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(training_generator.dataset),\n",
    "                100. * batch_idx / len(training_generator), loss.item()))\n",
    "# A simple test procedure to measure STN the performances on MNIST.\n",
    "#\n",
    "\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in validation_generator:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(validation_generator.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "              .format(test_loss, correct, len(validation_generator.dataset),\n",
    "                      100. * correct / len(validation_generator.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the STN results\n",
    "---------------------------\n",
    "\n",
    "Now, we will inspect the results of our learned visual attention\n",
    "mechanism.\n",
    "\n",
    "We define a small helper function in order to visualize the\n",
    "transformations while training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [8, 1, 7, 7], expected input[64, 3, 48, 48] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4b80af195c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-8aeaab324695>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-f06b408809c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# transform the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Perform the usual forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-f06b408809c5>\u001b[0m in \u001b[0;36mstn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Spatial transformer network forward function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 1, 7, 7], expected input[64, 3, 48, 48] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "def convert_image_np(inp):\n",
    "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "# We want to visualize the output of the spatial transformers layer\n",
    "# after the training, we visualize a batch of input images and\n",
    "# the corresponding transformed batch using STN.\n",
    "\n",
    "\n",
    "def visualize_stn():\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of training data\n",
    "        data = next(iter(test_loader))[0].to(device)\n",
    "\n",
    "        input_tensor = data.cpu()\n",
    "        transformed_input_tensor = model.stn(data).cpu()\n",
    "\n",
    "        in_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(input_tensor))\n",
    "\n",
    "        out_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(transformed_input_tensor))\n",
    "\n",
    "        # Plot the results side-by-side\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(in_grid)\n",
    "        axarr[0].set_title('Dataset Images')\n",
    "\n",
    "        axarr[1].imshow(out_grid)\n",
    "        axarr[1].set_title('Transformed Images')\n",
    "\n",
    "for epoch in range(1, 20 + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "# Visualize the STN transformation on some input batch\n",
    "visualize_stn()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
